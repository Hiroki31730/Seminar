{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMR6lqitOzh3dw+yzLADYdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hiroki31730/Seminar/blob/main/CNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Google colabではローカル環境にアクセスできないのでdiveと接続できるようにする\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZhPBqxSZicu",
        "outputId": "654d9c65-22e6-45ea-8175-81865977b81b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VR1L5SCXylNK"
      },
      "outputs": [],
      "source": [
        "# インポート\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# CNN用インポート\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, array_to_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習データとテストデータに分割\n",
        "D_path = '/content/drive/MyDrive'\n",
        "\n",
        "def Test_Set_CNN(rate): # rate:分割の割合\n",
        "    dir_ph = os.path.join(D_path, 'SANSIN_sp2')   # 元データのディレクトリパス\n",
        "    dir_Cnnph = os.path.join(D_path, 'CNN_sp_data')  # テストデータ、学習データの親ディレクトリパス\n",
        "    spl_tstph = os.path.join(dir_Cnnph, 'CNN_test_data') # テストデータディレクトリパス\n",
        "    spl_lrnph = os.path.join(dir_Cnnph, 'CNN_train_data') # 学習データディレクトリパス\n",
        "\n",
        "    # すでにディレクトリ(dir_Cnnph)が存在していれば全て削除\n",
        "    if os.path.isdir(dir_Cnnph):\n",
        "        shutil.rmtree(dir_Cnnph)\n",
        "\n",
        "    # ディレクトリ作成\n",
        "    os.makedirs(spl_tstph, exist_ok=True) # テストデータ\n",
        "    os.makedirs(spl_lrnph, exist_ok=True) # 学習データ\n",
        "\n",
        "    # 元データパス取得\n",
        "    dir_lists = os.listdir(dir_ph) # カテゴリー毎のディレクトリパス\n",
        "    # 隠しファイルを除外\n",
        "    dir_lists = [f for f in dir_lists if os.path.isdir(os.path.join(dir_ph, f))]\n",
        "\n",
        "    # コピー先のディレクトリ作成\n",
        "    ph_lst = []  # 元データのカテゴリー毎のディレクトリパスを格納\n",
        "    t_phlst = [] # カテゴリー毎のディレクトリパスを格納(テストデータ)\n",
        "    l_phlst = [] # カテゴリー毎のディレクトリパスを格納(学習データ)\n",
        "\n",
        "    # ディレクトリパス格納処理\n",
        "    for d_name in dir_lists: # d_name:カテゴリー名のパス\n",
        "        t_ph = os.path.join(spl_tstph, d_name)\n",
        "        l_ph = os.path.join(spl_lrnph, d_name)\n",
        "        os.makedirs(t_ph, exist_ok=True)\n",
        "        os.makedirs(l_ph, exist_ok=True)\n",
        "        t_phlst.append(t_ph)\n",
        "        l_phlst.append(l_ph)\n",
        "        ph_lst.append(os.path.join(dir_ph, d_name))\n",
        "\n",
        "    # 分割処理\n",
        "    for i,path in enumerate(ph_lst): # i: index番号, path:カテゴリー毎のディレクトリパス\n",
        "        f_name = os.listdir(path)   # カテゴリー内の画像ファイルパス取得\n",
        "        random.shuffle(f_name)      # ランダムに並び替え\n",
        "        rate_ind = int(len(f_name)*rate) # 分割の割合分のindex数\n",
        "\n",
        "        # 元データから学習データとして別ディレクトリに複製\n",
        "        for l_fn in f_name[rate_ind:]:\n",
        "            ori = os.path.join(path, l_fn)\n",
        "            cpy = os.path.join(l_phlst[i], l_fn)\n",
        "            shutil.copyfile(ori, cpy)\n",
        "\n",
        "        # 元データからテストデータとして別ディレクトリに複製\n",
        "        for t_fn in f_name[:rate_ind]:\n",
        "            ori = os.path.join(path, t_fn)\n",
        "            cpy = os.path.join(t_phlst[i], t_fn)\n",
        "            shutil.copyfile(ori, cpy)\n",
        "    print(\"データ分割完了\")\n",
        "    # カテゴリー毎に複製したテストデータ(t_phlst)と学習データ(l_phlst)のディレクトリパス\n",
        "    return t_phlst, l_phlst"
      ],
      "metadata": {
        "id": "w-4w3-pldr3n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習データ処理\n",
        "def Load_LrnFile(path, ct_n): # path:学習データのディレクトリパス, ct_n:カテゴリー数\n",
        "    img_data = [] # 画像の数値データ格納\n",
        "    cate = []     # カテゴリーのindex格納\n",
        "    cate_dct = {} # カテゴリーの並び順を格納\n",
        "\n",
        "    for typ_nm, t_dr in enumerate(path): # t_dr:カテゴリー毎のディレクトリパス\n",
        "        bs_nm = os.path.basename(t_dr) # カテゴリー名\n",
        "        cate_dct[bs_nm] = typ_nm       # index格納\n",
        "        f_lst = glob.glob(t_dr + \"/*.png\") # ディレクトリ内の全ファイルパス取得\n",
        "        for fn in f_lst:\n",
        "            img = load_img(fn) # 画像読み込み\n",
        "            img_arr = img_to_array(img) # 画像データを行列の数値データに変換\n",
        "            img_data.append(img_arr) # 格納\n",
        "            cate.append(typ_nm)      # カテゴリーのindex格納\n",
        "    img_data = np.array(img_data)    # ndarrayに変換\n",
        "    cate = np.array(cate)\n",
        "    img_data = img_data / 255.0      # 0.0〜1.0の範囲に正規化\n",
        "    cate = to_categorical(cate, ct_n) # 識別用のarrayに変換\n",
        "    IP_shape = img_data[0].shape  # 画像データのサイズ\n",
        "    return img_data, cate, cate_dct, IP_shape"
      ],
      "metadata": {
        "id": "PS5REUvuhg7w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テストデータ処理\n",
        "def Load_TestFile(path): # path:テストデータのディレクトリパス\n",
        "    img_data = [] # 画像の数値データ格納\n",
        "    file_nm = []  # ファイル名格納\n",
        "    for t_dr in path:\n",
        "        f_lst = glob.glob(t_dr + \"/*.png\") # ディレクトリ内の全ファイルパス取得\n",
        "        for fn in f_lst:\n",
        "            img = load_img(fn) # 画像読み込み\n",
        "            img_data.append(img_to_array(img)) # 画像データをndarrayの数値データに変換\n",
        "            file_nm.append(fn)\n",
        "    img_data = np.array(img_data)\n",
        "    file_nm = np.array(file_nm)\n",
        "    img_data = img_data / 255.0 # 正規化\n",
        "    return img_data, file_nm"
      ],
      "metadata": {
        "id": "SH-3Q-8RhkLr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# スペクトログラム_CNN学習\n",
        "# cate_n:カテゴリー数, lrn_rate:学習率, epo:エポック数, lrn_rate:学習データのディレクトリパス\n",
        "# mdl_ph:学習したmodelを保存するファイルパス\n",
        "def cnn_lrn(cate_n, lrn_rate, epo, lrn_path, mdl_ph):\n",
        "    img_data, cate, c_dct, IP_shape = Load_LrnFile(lrn_path, 4) #学習データ処理\n",
        "    model = keras.Sequential() # model宣言\n",
        "\n",
        "    # 畳み込みでカーネルを用いてテンソルを生成する層\n",
        "    conv = keras.layers.Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu', input_shape=IP_shape)\n",
        "    # プーリング層を追加\n",
        "    pool = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "    # データを平滑化(ノイズを目立たなくする)\n",
        "    flttn = keras.layers.Flatten()\n",
        "    # 全結合層\n",
        "    ct_Lyr = keras.layers.Dense(units = cate_n, activation='softmax') # units: 出力数=カテゴリー数, 'softmax'を使用\n",
        "\n",
        "    # 層追加\n",
        "    model.add(conv)\n",
        "    model.add(pool)\n",
        "    model.add(flttn)\n",
        "    model.add(ct_Lyr)\n",
        "\n",
        "    # 確率的勾配降下法\n",
        "    sgd = keras.optimizers.SGD(learning_rate=lrn_rate)\n",
        "    # model作成\n",
        "    model.compile(optimizer=sgd, loss=\"categorical_crossentropy\")\n",
        "    # modelの要約出力\n",
        "    model.summary()\n",
        "\n",
        "    #学習開始\n",
        "    model.fit(img_data, cate, epochs=epo)\n",
        "    # model保存(.h5)\n",
        "    model.save(mdl_ph)\n",
        "    print(\"学習完了\")\n",
        "    return c_dct"
      ],
      "metadata": {
        "id": "Q5EK5WVdhphe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 識別率を求める\n",
        "# n_l:カテゴリー名, alsn:種類ごとのデータ総数を格納した行列, altn:種類ごとの識別数を格納した行列, csv_ph:csvファイルパス\n",
        "def dsc_em(n_l, alsn, altn, csv_ph):\n",
        "    Rslt_all = 0 # 全体の識別率\n",
        "\n",
        "    Rslt = altn/alsn # 種類毎の識別率\n",
        "\n",
        "    # 全体の識別率を求める\n",
        "    for r in range(len(Rslt)):\n",
        "        Rslt_all = Rslt_all + Rslt[r][r]\n",
        "    Rslt_all = Rslt_all/len(Rslt)\n",
        "\n",
        "    # 種類毎の識別率を.csvに保存 (index:実際の値, column:識別された値)\n",
        "    R = pd.DataFrame(Rslt, index=n_l, columns=n_l)\n",
        "    R.to_csv(csv_ph)\n",
        "\n",
        "    # 結果表示\n",
        "    print(\"識別率\\n\" + str(Rslt_all*100) + \"%\")\n",
        "    print(R)\n",
        "    print()"
      ],
      "metadata": {
        "id": "SIje_XtNl3EC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 識別データ出力\n",
        "# csv_ph:識別結果を保存したcsvファイルパス, arg_nl:カテゴリー名を格納したlist\n",
        "def rslt_out(csv_ph, arg_nl):\n",
        "    arg_len = len(arg_nl) # カテゴリー数取得\n",
        "    jdg = pd.read_csv(csv_ph, index_col=[0]) # csv読み込み\n",
        "    rslt_altn = np.zeros((arg_len, arg_len)) # 種類毎の識別数の行列\n",
        "    rslt_alsn = np.zeros((arg_len, arg_len)) # 種類毎の全データ数の行列\n",
        "    for i_1 in range(len(arg_nl)):\n",
        "        rslt_tn = []\n",
        "        rslt_sn = []\n",
        "        for i_2 in arg_nl:\n",
        "            sn = (jdg[\"actual\"]==arg_nl[i_1]).sum()\n",
        "            tn = ((jdg[\"actual\"]==arg_nl[i_1])&(jdg[\"discrimination\"]==i_2)).sum()\n",
        "            rslt_tn.append(tn)\n",
        "            rslt_sn.append(sn)\n",
        "        rslt_altn[i_1] = rslt_altn[i_1] + rslt_tn\n",
        "        rslt_alsn[i_1] = rslt_alsn[i_1] + rslt_sn\n",
        "    return rslt_altn, rslt_alsn"
      ],
      "metadata": {
        "id": "iyqtbCdGl9OQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.zeros配列set\n",
        "def arr_zero(ln_0, ln_1):\n",
        "    allt=np.zeros((ln_0,ln_1))\n",
        "    alls=np.zeros((ln_0,ln_1))\n",
        "    return allt, alls"
      ],
      "metadata": {
        "id": "bRKwU5pcmCr5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 識別結果保存(.csv)\n",
        "# model_ph: modelを保存したファイルパス, tst_path:テストデータのディレクトリパス, cate_dct:カテゴリーの並び順を格納\n",
        "def cnn_dsc(model_ph, tst_path, cate_dct):\n",
        "    csv = \"CNN_result.csv\"  # 結果保存するファイルパス\n",
        "    csv_ph = D_path + \"/\" + csv\n",
        "    model = keras.models.load_model(model_ph) # model読み込み\n",
        "\n",
        "    tst_dlst, fn_lst = Load_TestFile(tst_path) # テストデータ処理\n",
        "    rslt = model.predict(tst_dlst) # 識別\n",
        "    rslt_df = pd.DataFrame(columns=[\"discrimination\", \"actual\", \"TorF\"]) # 識別結果格納DataFrame\n",
        "    # 格納\n",
        "    for r_i in range(len(rslt)):\n",
        "        r_ind = np.argmax(rslt[r_i]) # 識別結果の値のindex取得\n",
        "        dsc = [k for k, v in cate_dct.items() if v == r_ind][0] # indexを用いて識別結果取得\n",
        "        typ_nm = os.path.basename(os.path.dirname(fn_lst[r_i])) # ファイル名取得\n",
        "        # 格納\n",
        "        rslt_df.loc[os.path.basename(fn_lst[r_i])] = [dsc, typ_nm, bool(dsc == typ_nm)]\n",
        "    # 識別結果保存\n",
        "    rslt_df.to_csv(csv_ph)\n",
        "    return csv_ph"
      ],
      "metadata": {
        "id": "rR40L5Bhinyb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdl = \"lrn_model.h5\"\n",
        "cnn_Lrnrate = 0.01        # 学習率\n",
        "cnn_epo = 12             # epoch数\n",
        "Categories_name = ['ebony', 'overseas', 'maple_tree', 'ebony+leather']\n",
        "\n",
        "tst_ph, lrn_ph = Test_Set_CNN(0.1)   # 学習データとテストデータ分割"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRgduaNIocb7",
        "outputId": "22caee14-d668-444a-9cb1-1472d1116fcd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データ分割完了\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_epo = 12\n",
        "cate_dct = cnn_lrn(4, cnn_Lrnrate, cnn_epo, lrn_ph, mdl) # 学習"
      ],
      "metadata": {
        "id": "ebx5JbAz871A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050bb5b3-a8e8-4d32-97f6-01c2efbcc009"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 240, 320, 16)      1216      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 120, 160, 16)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 307200)            0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 1228804   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,230,020\n",
            "Trainable params: 1,230,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "57/57 [==============================] - 119s 2s/step - loss: 2.4018\n",
            "Epoch 2/12\n",
            "57/57 [==============================] - 115s 2s/step - loss: 1.3693\n",
            "Epoch 3/12\n",
            "57/57 [==============================] - 115s 2s/step - loss: 1.3328\n",
            "Epoch 4/12\n",
            "57/57 [==============================] - 115s 2s/step - loss: 1.2578\n",
            "Epoch 5/12\n",
            "57/57 [==============================] - 118s 2s/step - loss: 1.1496\n",
            "Epoch 6/12\n",
            "57/57 [==============================] - 116s 2s/step - loss: 1.0067\n",
            "Epoch 7/12\n",
            "57/57 [==============================] - 122s 2s/step - loss: 0.8915\n",
            "Epoch 8/12\n",
            "57/57 [==============================] - 116s 2s/step - loss: 0.7302\n",
            "Epoch 9/12\n",
            "57/57 [==============================] - 117s 2s/step - loss: 0.6575\n",
            "Epoch 10/12\n",
            "57/57 [==============================] - 120s 2s/step - loss: 0.5389\n",
            "Epoch 11/12\n",
            "57/57 [==============================] - 117s 2s/step - loss: 0.4712\n",
            "Epoch 12/12\n",
            "57/57 [==============================] - 117s 2s/step - loss: 0.4157\n",
            "学習完了\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_cnn = cnn_dsc(mdl, tst_ph, cate_dct)   # 識別\n",
        "altn_cnn, alsn_cnn = rslt_out(csv_cnn, Categories_name)  # 識別結果保存\n",
        "\n",
        "# 識別結果表示\n",
        "print()\n",
        "print(\"CNN\")\n",
        "dsc_em(Categories_name, alsn_cnn, altn_cnn, \"/content/drive/MyDrive/c_result.csv\")"
      ],
      "metadata": {
        "id": "3Ioo9X4a9NB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50614b46-1ee2-4c3c-ca9d-242935ec5c54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 6s 838ms/step\n",
            "\n",
            "CNN\n",
            "識別率\n",
            "79.08695652173911%\n",
            "                  ebony  overseas  maple_tree  ebony+leather\n",
            "ebony          1.000000  0.000000    0.000000       0.000000\n",
            "overseas       0.120000  0.860000    0.000000       0.020000\n",
            "maple_tree     0.391304  0.043478    0.543478       0.021739\n",
            "ebony+leather  0.240000  0.000000    0.000000       0.760000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイル数確認のためのコード\n",
        "s_path = os.path.join(D_path,'CNN_sp_data/CNN_train_data/ebony')\n",
        "\n",
        "ss = glob.glob(s_path + '/**')\n",
        "\n",
        "print(s_path)\n",
        "print(len(ss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "phZWjRlJ5mAq",
        "outputId": "edb7f0c8-3189-46b0-81fa-a20932768eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-54af14301323>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ファイル数確認のためのコード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CNN_sp_data/CNN_train_data/ebony'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/**'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    }
  ]
}